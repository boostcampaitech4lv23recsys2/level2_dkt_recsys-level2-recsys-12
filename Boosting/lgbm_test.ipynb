{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lgbm/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import optuna\n",
    "import optuna.integration.lightgbm as lgb\n",
    "\n",
    "from data_loader import lgbm_data_loader, category_data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_CUSTOM = True\n",
    "USE_VALID = True\n",
    "USE_PCA = False\n",
    "valid_len = 3\n",
    "n_components = 5\n",
    "\n",
    "DROPS = [\n",
    "    'Timestamp','year','day','minute','second','KnowledgeTag',\n",
    "]\n",
    "PCA = [\n",
    "    # 'hour_answerCode_sum',\n",
    "    # 'userID_dayofweek_answerCode_count',\n",
    "    # 'user_correct_answer',\n",
    "    # 'user_total_answer',\n",
    "    # 'hour_answerCode_var',\n",
    "    # 'hour_answerCode_mean',\n",
    "    # 'userID_first3_answerCode_count',\n",
    "    # 'userID_month_answerCode_count',\n",
    "    # 'KnowledgeTag_first3_answerCode_sum',\n",
    "    # 'KnowledgeTag',\n",
    "    # 'userID_answerCode_count',\n",
    "    # 'userID_answerCode_sum',\n",
    "    # 'testId_answerCode_sum',\n",
    "    # 'KnowledgeTag_answerCode_count',\n",
    "    # 'KnowledgeTag_answerCode_sum',\n",
    "    # 'month',\n",
    "    # 'hour',\n",
    "    # 'dayofweek',\n",
    "    # 'dayofweek_answerCode_mean',\n",
    "    # 'dayofweek_answerCode_count',\n",
    "    # 'dayofweek_answerCode_sum',\n",
    "    # 'dayofweek_answerCode_var',\n",
    "    # 'mid3',\n",
    "    # 'KnowledgeTag_first3_answerCode_mean',\n",
    "    # 'KnowledgeTag_first3_answerCode_count',\n",
    "    # 'month_answerCode_var',\n",
    "    # 'month_answerCode_count',\n",
    "    # 'hour_answerCode_count', \n",
    "    # 'KnowledgeTag_first3_answerCode_var',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label Encoding...: 100%|██████████| 69/69 [00:16<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start binning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning...: 100%|██████████| 20/20 [00:32<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# train, valid, y_train, y_valid, test = lgbm_data_loader(IS_CUSTOM=IS_CUSTOM,USE_VALID=USE_VALID, DROPS=DROPS, valid_len=valid_len)\n",
    "train, valid, y_train, y_valid, test, ids_of_categorical = category_data_loader(IS_CUSTOM=IS_CUSTOM, USE_VALID=USE_VALID, DROPS=DROPS, valid_len=valid_len,PCA=PCA, n_components=5, USE_PCA=USE_PCA)\n",
    "# train, valid, y_train, y_valid, test = ctb_data_loader(IS_CUSTOM=IS_CUSTOM,USE_VALID=USE_VALID, DROPS=DROPS+low_importance, low_importance=low_importance, n_components=5, valid_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-30 13:49:12,971]\u001b[0m A new study created in memory with name: no-name-2344c2dd-e8ac-4a5f-af8a-147028611475\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]/opt/conda/envs/lgbm/lib/python3.10/site-packages/lightgbm/engine.py:577: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/envs/lgbm/lib/python3.10/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['KnowledgeTag_answerCode_count', 'KnowledgeTag_answerCode_mean', 'KnowledgeTag_answerCode_sum', 'KnowledgeTag_answerCode_var', 'KnowledgeTag_first3_answerCode_count', 'KnowledgeTag_first3_answerCode_mean', 'KnowledgeTag_first3_answerCode_sum', 'KnowledgeTag_first3_answerCode_var', 'KnowledgeTag_first3_clust', 'assessmentItemID', 'assessmentItemID_answerCode_count', 'assessmentItemID_answerCode_mean', 'assessmentItemID_answerCode_sum', 'assessmentItemID_answerCode_var', 'dayofweek', 'dayofweek_answerCode_count', 'dayofweek_answerCode_mean', 'dayofweek_answerCode_sum', 'dayofweek_answerCode_var', 'first3', 'hour', 'hour_answerCode_Level', 'hour_answerCode_count', 'hour_answerCode_mean', 'hour_answerCode_sum', 'hour_answerCode_var', 'last3', 'mid3', 'month', 'month_answerCode_count', 'month_answerCode_mean', 'month_answerCode_sum', 'month_answerCode_var', 'testId', 'testId_answerCode_count', 'testId_answerCode_mean', 'testId_answerCode_sum', 'testId_answerCode_var', 'userID', 'userID_answerCode_count', 'userID_answerCode_mean', 'userID_answerCode_sum', 'userID_answerCode_var', 'userID_dayofweek_answerCode_count', 'userID_dayofweek_answerCode_mean', 'userID_dayofweek_answerCode_sum', 'userID_dayofweek_answerCode_var', 'userID_first3_answerCode_count', 'userID_first3_answerCode_mean', 'userID_first3_answerCode_sum', 'userID_first3_answerCode_var', 'userID_hour_answerCode_count', 'userID_hour_answerCode_mean', 'userID_hour_answerCode_sum', 'userID_hour_answerCode_var', 'userID_month_answerCode_count', 'userID_month_answerCode_mean', 'userID_month_answerCode_sum', 'userID_month_answerCode_var', 'user_acc', 'user_correct_answer', 'user_total_answer']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "/opt/conda/envs/lgbm/lib/python3.10/site-packages/lightgbm/engine.py:620: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[33m[W 2022-11-30 14:06:35,753]\u001b[0m Trial 0 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/lgbm/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/opt/conda/envs/lgbm/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py\", line 319, in __call__\n",
      "    cv_results = lgb.cv(self.lgbm_params, train_set, **self.lgbm_kwargs)\n",
      "  File \"/opt/conda/envs/lgbm/lib/python3.10/site-packages/lightgbm/engine.py\", line 640, in cv\n",
      "    cvfolds.update(fobj=fobj)\n",
      "  File \"/opt/conda/envs/lgbm/lib/python3.10/site-packages/lightgbm/engine.py\", line 353, in handler_function\n",
      "    ret.append(getattr(booster, name)(*args, **kwargs))\n",
      "  File \"/opt/conda/envs/lgbm/lib/python3.10/site-packages/lightgbm/basic.py\", line 3021, in update\n",
      "    _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 34\u001b[0m\n\u001b[1;32m     19\u001b[0m optuna\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39mset_verbosity(optuna\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39mWARNING) \n\u001b[1;32m     21\u001b[0m tuner \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mLightGBMTunerCV(params, \n\u001b[1;32m     22\u001b[0m                             dtrain, \n\u001b[1;32m     23\u001b[0m                             categorical_feature\u001b[39m=\u001b[39mids_of_categorical,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m                             callbacks\u001b[39m=\u001b[39m[lgb\u001b[39m.\u001b[39mreset_parameter(learning_rate \u001b[39m=\u001b[39m [\u001b[39m0.005\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m200\u001b[39m \u001b[39m+\u001b[39m [\u001b[39m0.001\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m9800\u001b[39m) ] \u001b[39m#[0.1]*5 + [0.05]*15 + [0.01]*45 + \u001b[39;00m\n\u001b[1;32m     32\u001b[0m                            )\n\u001b[0;32m---> 34\u001b[0m tuner\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/opt/conda/envs/lgbm/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:545\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39m# Sampling.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_train_set()\n\u001b[0;32m--> 545\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtune_feature_fraction()\n\u001b[1;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtune_num_leaves()\n\u001b[1;32m    547\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtune_bagging()\n",
      "File \u001b[0;32m/opt/conda/envs/lgbm/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:570\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner.tune_feature_fraction\u001b[0;34m(self, n_trials)\u001b[0m\n\u001b[1;32m    567\u001b[0m param_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m0.4\u001b[39m, \u001b[39m1.0\u001b[39m, n_trials)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    569\u001b[0m sampler \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mGridSampler({param_name: param_values})\n\u001b[0;32m--> 570\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tune_params([param_name], \u001b[39mlen\u001b[39;49m(param_values), sampler, \u001b[39m\"\u001b[39;49m\u001b[39mfeature_fraction\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/envs/lgbm/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:653\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner._tune_params\u001b[0;34m(self, target_param_names, n_trials, sampler, step_name)\u001b[0m\n\u001b[1;32m    651\u001b[0m     _timeout \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[39mif\u001b[39;00m _n_trials \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 653\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    654\u001b[0m         objective,\n\u001b[1;32m    655\u001b[0m         n_trials\u001b[39m=\u001b[39;49m_n_trials,\n\u001b[1;32m    656\u001b[0m         timeout\u001b[39m=\u001b[39;49m_timeout,\n\u001b[1;32m    657\u001b[0m         catch\u001b[39m=\u001b[39;49m(),\n\u001b[1;32m    658\u001b[0m         callbacks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optuna_callbacks,\n\u001b[1;32m    659\u001b[0m     )\n\u001b[1;32m    661\u001b[0m \u001b[39mif\u001b[39;00m pbar:\n\u001b[1;32m    662\u001b[0m     pbar\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/envs/lgbm/lib/python3.10/site-packages/optuna/study/study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     _optimize(\n\u001b[1;32m    420\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    421\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    422\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    423\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    424\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    425\u001b[0m         catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[1;32m    426\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    427\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    428\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/lgbm/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/lgbm/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/conda/envs/lgbm/lib/python3.10/site-packages/optuna/study/_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    231\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    233\u001b[0m ):\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/conda/envs/lgbm/lib/python3.10/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m/opt/conda/envs/lgbm/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:319\u001b[0m, in \u001b[0;36m_OptunaObjectiveCV.__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    317\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    318\u001b[0m train_set \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_set)\n\u001b[0;32m--> 319\u001b[0m cv_results \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mcv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlgbm_params, train_set, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlgbm_kwargs)\n\u001b[1;32m    321\u001b[0m val_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_cv_scores(cv_results)\n\u001b[1;32m    322\u001b[0m val_score \u001b[39m=\u001b[39m val_scores[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/lgbm/lib/python3.10/site-packages/lightgbm/engine.py:640\u001b[0m, in \u001b[0;36mcv\u001b[0;34m(params, train_set, num_boost_round, folds, nfold, stratified, shuffle, metrics, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, fpreproc, verbose_eval, show_stdv, seed, callbacks, eval_train_metric, return_cvbooster)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    634\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mcvfolds,\n\u001b[1;32m    635\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    636\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m    637\u001b[0m                             begin_iteration\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m    638\u001b[0m                             end_iteration\u001b[39m=\u001b[39mnum_boost_round,\n\u001b[1;32m    639\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 640\u001b[0m cvfolds\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[1;32m    641\u001b[0m res \u001b[39m=\u001b[39m _agg_cv_result(cvfolds\u001b[39m.\u001b[39meval_valid(feval), eval_train_metric)\n\u001b[1;32m    642\u001b[0m \u001b[39mfor\u001b[39;00m _, key, mean, _, std \u001b[39min\u001b[39;00m res:\n",
      "File \u001b[0;32m/opt/conda/envs/lgbm/lib/python3.10/site-packages/lightgbm/engine.py:353\u001b[0m, in \u001b[0;36mCVBooster.__getattr__.<locals>.handler_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m ret \u001b[39m=\u001b[39m []\n\u001b[1;32m    352\u001b[0m \u001b[39mfor\u001b[39;00m booster \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboosters:\n\u001b[0;32m--> 353\u001b[0m     ret\u001b[39m.\u001b[39mappend(\u001b[39mgetattr\u001b[39;49m(booster, name)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    354\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/conda/envs/lgbm/lib/python3.10/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[1;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=42)\n",
    "\n",
    "params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_error\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",                \n",
    "        \"seed\": 42\n",
    "    }\n",
    "\n",
    "X = train    \n",
    "y = y_train\n",
    "\n",
    "study_tuner = optuna.create_study(direction='minimize')\n",
    "dtrain = lgb.Dataset(X, label=y)\n",
    "\n",
    "# Suppress information only outputs - otherwise optuna is \n",
    "# quite verbose, which can be nice, but takes up a lot of space\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
    "\n",
    "tuner = lgb.LightGBMTunerCV(params, \n",
    "                            dtrain, \n",
    "                            categorical_feature=ids_of_categorical,\n",
    "                            study=study_tuner,\n",
    "                            verbose_eval=False,                            \n",
    "                            early_stopping_rounds=250,\n",
    "                            time_budget=19800, # Time budget of 5 hours, we will not really need it\n",
    "                            seed = 42,\n",
    "                            folds=rkf,\n",
    "                            num_boost_round=10000,\n",
    "                            callbacks=[lgb.reset_parameter(learning_rate = [0.005]*200 + [0.001]*9800) ] #[0.1]*5 + [0.05]*15 + [0.01]*45 + \n",
    "                           )\n",
    "\n",
    "tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuner.best_params)\n",
    "# Classification error\n",
    "print(tuner.best_score)\n",
    "# Or expressed as accuracy\n",
    "print(1.0-tuner.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_best_params = tuner.best_params\n",
    "\n",
    "if tmp_best_params['feature_fraction']==1:\n",
    "    tmp_best_params['feature_fraction']=1.0-1e-9\n",
    "if tmp_best_params['feature_fraction']==0:\n",
    "    tmp_best_params['feature_fraction']=1e-9\n",
    "if tmp_best_params['bagging_fraction']==1:\n",
    "    tmp_best_params['bagging_fraction']=1.0-1e-9\n",
    "if tmp_best_params['bagging_fraction']==0:\n",
    "    tmp_best_params['bagging_fraction']=1e-9  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "dtrain = lgb.Dataset(X, label=y)\n",
    "\n",
    "# We will track how many training rounds we needed for our best score.\n",
    "# We will use that number of rounds later.\n",
    "best_score = 999\n",
    "training_rounds = 10000\n",
    "\n",
    "# Declare how we evaluate how good a set of hyperparameters are, i.e.\n",
    "# declare an objective function.\n",
    "def objective(trial):\n",
    "    # Specify a search space using distributions across plausible values of hyperparameters.\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_error\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",                \n",
    "        \"seed\": 42,\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 0, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n",
    "        'seed': 1979\n",
    "    }\n",
    "    lgbcv = lgb.cv(param,\n",
    "                   dtrain,\n",
    "                   categorical_feature=ids_of_categorical,\n",
    "                   folds=rkf,\n",
    "                   verbose_eval=False,                   \n",
    "                   early_stopping_rounds=250,                   \n",
    "                   num_boost_round=10000,                    \n",
    "                   callbacks=[lgb.reset_parameter(learning_rate = [0.005]*200 + [0.001]*9800) ]\n",
    "                  )\n",
    "    \n",
    "    cv_score = lgbcv['binary_error-mean'][-1] + lgbcv['binary_error-stdv'][-1]\n",
    "    if cv_score<best_score:\n",
    "        training_rounds = len( list(lgbcv.values())[0] )\n",
    "    \n",
    "    # Return metric of interest\n",
    "    return cv_score\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
    "\n",
    "# We search for another 4 hours (3600 s are an hours, so timeout=14400).\n",
    "# We could instead do e.g. n_trials=1000, to try 1000 hyperparameters chosen \n",
    "# by optuna or set neither timeout or n_trials so that we keep going until \n",
    "# the user interrupts (\"Cancel run\").\n",
    "study = optuna.create_study(direction='maximize')  \n",
    "study.enqueue_trial(tmp_best_params)\n",
    "study.optimize(objective, timeout=14400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('lgbm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04807071a00f9300ed95f120ffebd1294355433dc954c3167b4b6b21e7a5974a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
