{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt7sDAqHhfQp"
      },
      "source": [
        "# LGBM을 활용한 베이스라인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.375544Z",
          "start_time": "2021-05-24T09:49:28.999092Z"
        },
        "id": "Uq_TJqbdhfQu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "from optuna.samplers import TPESampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZlm5HSmhfQv"
      },
      "source": [
        "## 1. 데이터 로딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.678737Z",
          "start_time": "2021-05-24T09:49:29.376581Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "s6qgJ8MLhfQw",
        "outputId": "75ca07b2-1b41-4913-e62f-6a8f857d7750"
      },
      "outputs": [],
      "source": [
        "data_dir = '/opt/ml/input/data/' # 경로는 상황에 맞춰서 수정해주세요!\n",
        "csv_file_path = os.path.join(data_dir, 'train_data.csv') # 데이터는 대회홈페이지에서 받아주세요 :)\n",
        "df = pd.read_csv(csv_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_oCGAgEhfQw"
      },
      "source": [
        "## 2. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.682739Z",
          "start_time": "2021-05-24T09:49:28.979Z"
        },
        "id": "URNLoukChfQx"
      },
      "outputs": [],
      "source": [
        "def feature_engineering(df):\n",
        "    \n",
        "    #유저별 시퀀스를 고려하기 위해 아래와 같이 정렬\n",
        "    df.sort_values(by=['userID','Timestamp'], inplace=True)\n",
        "    \n",
        "    #유저들의 문제 풀이수, 정답 수, 정답률을 시간순으로 누적해서 계산\n",
        "    df['user_correct_answer'] = df.groupby('userID')['answerCode'].transform(lambda x: x.cumsum().shift(1))\n",
        "    df['user_total_answer'] = df.groupby('userID')['answerCode'].cumcount()\n",
        "    df['user_acc'] = df['user_correct_answer']/df['user_total_answer']\n",
        "\n",
        "    # testId와 KnowledgeTag의 전체 정답률은 한번에 계산\n",
        "    # 아래 데이터는 제출용 데이터셋에 대해서도 재사용\n",
        "    correct_t = df.groupby(['testId'])['answerCode'].agg(['mean', 'sum'])\n",
        "    correct_t.columns = [\"test_mean\", 'test_sum']\n",
        "    correct_k = df.groupby(['KnowledgeTag'])['answerCode'].agg(['mean', 'sum'])\n",
        "    correct_k.columns = [\"tag_mean\", 'tag_sum']\n",
        "\n",
        "    df = pd.merge(df, correct_t, on=['testId'], how=\"left\")\n",
        "    df = pd.merge(df, correct_k, on=['KnowledgeTag'], how=\"left\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.683739Z",
          "start_time": "2021-05-24T09:49:28.981Z"
        },
        "id": "2vsUwksMhfQy",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>assessmentItemID</th>\n",
              "      <th>testId</th>\n",
              "      <th>answerCode</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>KnowledgeTag</th>\n",
              "      <th>user_correct_answer</th>\n",
              "      <th>user_total_answer</th>\n",
              "      <th>user_acc</th>\n",
              "      <th>test_mean</th>\n",
              "      <th>test_sum</th>\n",
              "      <th>tag_mean</th>\n",
              "      <th>tag_sum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001001</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:11</td>\n",
              "      <td>7224</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.947683</td>\n",
              "      <td>1268</td>\n",
              "      <td>0.955022</td>\n",
              "      <td>637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001002</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:14</td>\n",
              "      <td>7225</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.947683</td>\n",
              "      <td>1268</td>\n",
              "      <td>0.913187</td>\n",
              "      <td>3040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001003</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:22</td>\n",
              "      <td>7225</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.947683</td>\n",
              "      <td>1268</td>\n",
              "      <td>0.913187</td>\n",
              "      <td>3040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001004</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:29</td>\n",
              "      <td>7225</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.947683</td>\n",
              "      <td>1268</td>\n",
              "      <td>0.913187</td>\n",
              "      <td>3040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001005</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:36</td>\n",
              "      <td>7225</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.947683</td>\n",
              "      <td>1268</td>\n",
              "      <td>0.913187</td>\n",
              "      <td>3040</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userID assessmentItemID      testId  answerCode            Timestamp  \\\n",
              "0       0       A060001001  A060000001           1  2020-03-24 00:17:11   \n",
              "1       0       A060001002  A060000001           1  2020-03-24 00:17:14   \n",
              "2       0       A060001003  A060000001           1  2020-03-24 00:17:22   \n",
              "3       0       A060001004  A060000001           1  2020-03-24 00:17:29   \n",
              "4       0       A060001005  A060000001           1  2020-03-24 00:17:36   \n",
              "\n",
              "   KnowledgeTag  user_correct_answer  user_total_answer  user_acc  test_mean  \\\n",
              "0          7224                  NaN                  0       NaN   0.947683   \n",
              "1          7225                  1.0                  1       1.0   0.947683   \n",
              "2          7225                  2.0                  2       1.0   0.947683   \n",
              "3          7225                  3.0                  3       1.0   0.947683   \n",
              "4          7225                  4.0                  4       1.0   0.947683   \n",
              "\n",
              "   test_sum  tag_mean  tag_sum  \n",
              "0      1268  0.955022      637  \n",
              "1      1268  0.913187     3040  \n",
              "2      1268  0.913187     3040  \n",
              "3      1268  0.913187     3040  \n",
              "4      1268  0.913187     3040  "
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = feature_engineering(df)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VZzei3DhfQy"
      },
      "source": [
        "## 3. Train/Test 데이터 셋 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.684739Z",
          "start_time": "2021-05-24T09:49:28.982Z"
        },
        "id": "YOPWK7ckhfQz"
      },
      "outputs": [],
      "source": [
        "# train과 test 데이터셋은 사용자 별로 묶어서 분리를 해주어야함\n",
        "random.seed(42)\n",
        "def custom_train_test_split(df, ratio=0.7, split=True):\n",
        "    \n",
        "    users = list(zip(df['userID'].value_counts().index, df['userID'].value_counts()))\n",
        "    random.shuffle(users)\n",
        "    \n",
        "    max_train_data_len = ratio*len(df)\n",
        "    sum_of_train_data = 0\n",
        "    user_ids =[]\n",
        "\n",
        "    for user_id, count in users:\n",
        "        sum_of_train_data += count\n",
        "        if max_train_data_len < sum_of_train_data:\n",
        "            break\n",
        "        user_ids.append(user_id)\n",
        "\n",
        "\n",
        "    train = df[df['userID'].isin(user_ids)]\n",
        "    test = df[df['userID'].isin(user_ids) == False]\n",
        "\n",
        "    #test데이터셋은 각 유저의 마지막 interaction만 추출\n",
        "    test = test[test['userID'] != test['userID'].shift(-1)]\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.686739Z",
          "start_time": "2021-05-24T09:49:28.984Z"
        },
        "id": "i3HzdoybhfQ0"
      },
      "outputs": [],
      "source": [
        "# 유저별 분리\n",
        "train, test = custom_train_test_split(df)\n",
        "\n",
        "# 사용할 Feature 설정\n",
        "FEATS = ['KnowledgeTag', 'user_correct_answer', 'user_total_answer', \n",
        "         'user_acc', 'test_mean', 'test_sum', 'tag_mean','tag_sum']\n",
        "\n",
        "# X, y 값 분리\n",
        "y_train = train['answerCode']\n",
        "train = train.drop(['answerCode'], axis=1)\n",
        "\n",
        "y_test = test['answerCode']\n",
        "test = test.drop(['answerCode'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "Y4c-gjS2hfQ0"
      },
      "outputs": [],
      "source": [
        "# !pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.687739Z",
          "start_time": "2021-05-24T09:49:28.985Z"
        },
        "id": "mwAF79FbhfQ1"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.688736Z",
          "start_time": "2021-05-24T09:49:28.986Z"
        },
        "id": "o59GhDdVhfQ1"
      },
      "outputs": [],
      "source": [
        "lgb_train = lgb.Dataset(train[FEATS], y_train)\n",
        "lgb_test = lgb.Dataset(test[FEATS], y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 하이퍼파라미터 튜닝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiHss_BBhfQ2"
      },
      "source": [
        "## 4. 훈련 및 검증 (+ 하이퍼파라미터 튜닝(optuna))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.689738Z",
          "start_time": "2021-05-24T09:49:28.988Z"
        },
        "id": "-6FZfjA_hfQ2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-25 17:56:30,819]\u001b[0m A new study created in memory with name: lgbm_parameter_opt\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 1039565, number of negative: 546592\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007995 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2031\n",
            "[LightGBM] [Info] Number of data points in the train set: 1586157, number of used features: 8\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655399 -> initscore=0.642855\n",
            "[LightGBM] [Info] Start training from score 0.642855\n",
            "Training until validation scores don't improve for 10 rounds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-25 17:56:34,596]\u001b[0m Trial 0 finished with value: 0.6802742993440667 and parameters: {'bagging_fraction': 0.54, 'lr': 0.001, 'n_iter': 100, 'max_depth': -1, 'patience': 10}. Best is trial 0 with value: 0.6802742993440667.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's binary_logloss: 0.630835\tvalid_1's binary_logloss: 0.74029\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\ttraining's binary_logloss: 0.630835\tvalid_1's binary_logloss: 0.74029\n",
            "[LightGBM] [Info] Number of positive: 1039565, number of negative: 546592\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007675 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2031\n",
            "[LightGBM] [Info] Number of data points in the train set: 1586157, number of used features: 8\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655399 -> initscore=0.642855\n",
            "[LightGBM] [Info] Start training from score 0.642855\n",
            "Training until validation scores don't improve for 10 rounds\n",
            "[100]\ttraining's binary_logloss: 0.604284\tvalid_1's binary_logloss: 0.714731\n",
            "[200]\ttraining's binary_logloss: 0.587834\tvalid_1's binary_logloss: 0.699589\n",
            "[300]\ttraining's binary_logloss: 0.579609\tvalid_1's binary_logloss: 0.692499\n",
            "[400]\ttraining's binary_logloss: 0.575022\tvalid_1's binary_logloss: 0.68912\n",
            "[500]\ttraining's binary_logloss: 0.572282\tvalid_1's binary_logloss: 0.687455\n",
            "[600]\ttraining's binary_logloss: 0.570565\tvalid_1's binary_logloss: 0.686622\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-25 17:56:51,883]\u001b[0m Trial 1 finished with value: 0.6831847545219638 and parameters: {'bagging_fraction': 0.52, 'lr': 0.01, 'n_iter': 700, 'max_depth': 1, 'patience': 10}. Best is trial 1 with value: 0.6831847545219638.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[700]\ttraining's binary_logloss: 0.56945\tvalid_1's binary_logloss: 0.686116\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[700]\ttraining's binary_logloss: 0.56945\tvalid_1's binary_logloss: 0.686116\n",
            "[LightGBM] [Info] Number of positive: 1039565, number of negative: 546592\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007762 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2031\n",
            "[LightGBM] [Info] Number of data points in the train set: 1586157, number of used features: 8\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655399 -> initscore=0.642855\n",
            "[LightGBM] [Info] Start training from score 0.642855\n",
            "Training until validation scores don't improve for 5 rounds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-25 17:56:53,354]\u001b[0m Trial 2 finished with value: 0.6880669846948917 and parameters: {'bagging_fraction': 0.56, 'lr': 0.1, 'n_iter': 1000, 'max_depth': -1, 'patience': 5}. Best is trial 2 with value: 0.6880669846948917.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[36]\ttraining's binary_logloss: 0.562779\tvalid_1's binary_logloss: 0.682851\n",
            "[LightGBM] [Info] Number of positive: 1039565, number of negative: 546592\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007577 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2031\n",
            "[LightGBM] [Info] Number of data points in the train set: 1586157, number of used features: 8\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655399 -> initscore=0.642855\n",
            "[LightGBM] [Info] Start training from score 0.642855\n",
            "Training until validation scores don't improve for 5 rounds\n",
            "[100]\ttraining's binary_logloss: 0.637805\tvalid_1's binary_logloss: 0.74749\n",
            "[200]\ttraining's binary_logloss: 0.632429\tvalid_1's binary_logloss: 0.742155\n",
            "[300]\ttraining's binary_logloss: 0.627607\tvalid_1's binary_logloss: 0.737391\n",
            "[400]\ttraining's binary_logloss: 0.623266\tvalid_1's binary_logloss: 0.733155\n",
            "[500]\ttraining's binary_logloss: 0.61934\tvalid_1's binary_logloss: 0.729277\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-25 17:57:07,385]\u001b[0m Trial 3 finished with value: 0.6619310276287022 and parameters: {'bagging_fraction': 0.5, 'lr': 0.001, 'n_iter': 600, 'max_depth': 1, 'patience': 5}. Best is trial 2 with value: 0.6880669846948917.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[600]\ttraining's binary_logloss: 0.61578\tvalid_1's binary_logloss: 0.725826\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[600]\ttraining's binary_logloss: 0.61578\tvalid_1's binary_logloss: 0.725826\n",
            "[LightGBM] [Info] Number of positive: 1039565, number of negative: 546592\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007866 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2031\n",
            "[LightGBM] [Info] Number of data points in the train set: 1586157, number of used features: 8\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655399 -> initscore=0.642855\n",
            "[LightGBM] [Info] Start training from score 0.642855\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[100]\ttraining's binary_logloss: 0.561524\tvalid_1's binary_logloss: 0.683336\n",
            "[200]\ttraining's binary_logloss: 0.559385\tvalid_1's binary_logloss: 0.681498\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-25 17:57:15,010]\u001b[0m Trial 4 finished with value: 0.6897207314649175 and parameters: {'bagging_fraction': 0.5, 'lr': 0.05, 'n_iter': 900, 'max_depth': -1, 'patience': 20}. Best is trial 4 with value: 0.6897207314649175.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[220]\ttraining's binary_logloss: 0.559046\tvalid_1's binary_logloss: 0.681123\n",
            "[LightGBM] [Info] Number of positive: 1039565, number of negative: 546592\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006861 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2031\n",
            "[LightGBM] [Info] Number of data points in the train set: 1586157, number of used features: 8\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655399 -> initscore=0.642855\n",
            "[LightGBM] [Info] Start training from score 0.642855\n",
            "Training until validation scores don't improve for 5 rounds\n",
            "[100]\ttraining's binary_logloss: 0.604284\tvalid_1's binary_logloss: 0.714731\n",
            "[200]\ttraining's binary_logloss: 0.587834\tvalid_1's binary_logloss: 0.699589\n",
            "[300]\ttraining's binary_logloss: 0.579609\tvalid_1's binary_logloss: 0.692499\n",
            "[400]\ttraining's binary_logloss: 0.575022\tvalid_1's binary_logloss: 0.68912\n",
            "[500]\ttraining's binary_logloss: 0.572282\tvalid_1's binary_logloss: 0.687455\n",
            "[600]\ttraining's binary_logloss: 0.570565\tvalid_1's binary_logloss: 0.686622\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-25 17:57:31,262]\u001b[0m Trial 5 finished with value: 0.6830714569668057 and parameters: {'bagging_fraction': 0.58, 'lr': 0.01, 'n_iter': 800, 'max_depth': 1, 'patience': 5}. Best is trial 4 with value: 0.6897207314649175.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[673]\ttraining's binary_logloss: 0.569708\tvalid_1's binary_logloss: 0.686239\n",
            "[LightGBM] [Info] Number of positive: 1039565, number of negative: 546592\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007703 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2031\n",
            "[LightGBM] [Info] Number of data points in the train set: 1586157, number of used features: 8\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655399 -> initscore=0.642855\n",
            "[LightGBM] [Info] Start training from score 0.642855\n",
            "Training until validation scores don't improve for 5 rounds\n",
            "[100]\ttraining's binary_logloss: 0.572103\tvalid_1's binary_logloss: 0.687404\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-25 17:57:35,525]\u001b[0m Trial 6 finished with value: 0.6836409262572054 and parameters: {'bagging_fraction': 0.53, 'lr': 0.05, 'n_iter': 200, 'max_depth': 1, 'patience': 5}. Best is trial 4 with value: 0.6897207314649175.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[166]\ttraining's binary_logloss: 0.568453\tvalid_1's binary_logloss: 0.685447\n",
            "[LightGBM] [Info] Number of positive: 1039565, number of negative: 546592\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008257 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2031\n",
            "[LightGBM] [Info] Number of data points in the train set: 1586157, number of used features: 8\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655399 -> initscore=0.642855\n",
            "[LightGBM] [Info] Start training from score 0.642855\n",
            "[100]\ttraining's binary_logloss: 0.604284\tvalid_1's binary_logloss: 0.714731\n",
            "[200]\ttraining's binary_logloss: 0.587834\tvalid_1's binary_logloss: 0.699589\n",
            "[300]\ttraining's binary_logloss: 0.579609\tvalid_1's binary_logloss: 0.692499\n",
            "[400]\ttraining's binary_logloss: 0.575022\tvalid_1's binary_logloss: 0.68912\n",
            "[500]\ttraining's binary_logloss: 0.572282\tvalid_1's binary_logloss: 0.687455\n",
            "[600]\ttraining's binary_logloss: 0.570565\tvalid_1's binary_logloss: 0.686622\n",
            "[700]\ttraining's binary_logloss: 0.56945\tvalid_1's binary_logloss: 0.686116\n",
            "[800]\ttraining's binary_logloss: 0.568696\tvalid_1's binary_logloss: 0.685703\n",
            "[900]\ttraining's binary_logloss: 0.568167\tvalid_1's binary_logloss: 0.685323\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-25 17:58:00,280]\u001b[0m Trial 7 finished with value: 0.6842213277678394 and parameters: {'bagging_fraction': 0.5, 'lr': 0.01, 'n_iter': 1000, 'max_depth': 1, 'patience': 0}. Best is trial 4 with value: 0.6897207314649175.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1000]\ttraining's binary_logloss: 0.567772\tvalid_1's binary_logloss: 0.684876\n",
            "[LightGBM] [Info] Number of positive: 1039565, number of negative: 546592\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079425 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2031\n",
            "[LightGBM] [Info] Number of data points in the train set: 1586157, number of used features: 8\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655399 -> initscore=0.642855\n",
            "[LightGBM] [Info] Start training from score 0.642855\n",
            "[100]\ttraining's binary_logloss: 0.604284\tvalid_1's binary_logloss: 0.714731\n",
            "[200]\ttraining's binary_logloss: 0.587834\tvalid_1's binary_logloss: 0.699589\n",
            "[300]\ttraining's binary_logloss: 0.579609\tvalid_1's binary_logloss: 0.692499\n",
            "[400]\ttraining's binary_logloss: 0.575022\tvalid_1's binary_logloss: 0.68912\n",
            "[500]\ttraining's binary_logloss: 0.572282\tvalid_1's binary_logloss: 0.687455\n",
            "[600]\ttraining's binary_logloss: 0.570565\tvalid_1's binary_logloss: 0.686622\n",
            "[700]\ttraining's binary_logloss: 0.56945\tvalid_1's binary_logloss: 0.686116\n",
            "[800]\ttraining's binary_logloss: 0.568696\tvalid_1's binary_logloss: 0.685703\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-25 17:58:24,128]\u001b[0m Trial 8 finished with value: 0.6837885112303717 and parameters: {'bagging_fraction': 0.6, 'lr': 0.01, 'n_iter': 900, 'max_depth': 1, 'patience': 0}. Best is trial 4 with value: 0.6897207314649175.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[900]\ttraining's binary_logloss: 0.568167\tvalid_1's binary_logloss: 0.685323\n",
            "[LightGBM] [Info] Number of positive: 1039565, number of negative: 546592\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006973 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2031\n",
            "[LightGBM] [Info] Number of data points in the train set: 1586157, number of used features: 8\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655399 -> initscore=0.642855\n",
            "[LightGBM] [Info] Start training from score 0.642855\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[100]\ttraining's binary_logloss: 0.637805\tvalid_1's binary_logloss: 0.74749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-11-25 17:58:28,811]\u001b[0m Trial 9 finished with value: 0.6530426356589147 and parameters: {'bagging_fraction': 0.58, 'lr': 0.001, 'n_iter': 200, 'max_depth': 1, 'patience': 20}. Best is trial 4 with value: 0.6897207314649175.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[200]\ttraining's binary_logloss: 0.632429\tvalid_1's binary_logloss: 0.742155\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[200]\ttraining's binary_logloss: 0.632429\tvalid_1's binary_logloss: 0.742155\n",
            "Best Score: 0.6897207314649175\n",
            "Best trial: {'bagging_fraction': 0.5, 'lr': 0.05, 'n_iter': 900, 'max_depth': -1, 'patience': 20}\n"
          ]
        }
      ],
      "source": [
        "def objective(trial: Trial):\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'bagging_fraction': trial.suggest_float(\"bagging_fraction\", 0.5, 0.6, step=0.01),\n",
        "        'bagging_seed': 11, ##\n",
        "        'learning_rate': trial.suggest_categorical(\"lr\", [0.001, 0.005, 0.01, 0.05, 0.1]),\n",
        "        'num_iterations': trial.suggest_int(\"n_iter\", 100, 1000, 100),\n",
        "        'max_depth': trial.suggest_categorical('max_depth', [-1, 1]), # need to consider\n",
        "        'boosting': 'gbdt',\n",
        "        'early_stopping': trial.suggest_categorical('patience', [0, 5, 10, 15, 20])\n",
        "\n",
        "    }\n",
        "    model = lgb.train(\n",
        "        params, \n",
        "        lgb_train,\n",
        "        valid_sets=[lgb_train, lgb_test],\n",
        "        verbose_eval=100,\n",
        "        num_boost_round=500,\n",
        "        early_stopping_rounds=100\n",
        "    )\n",
        "\n",
        "    preds = model.predict(test[FEATS])\n",
        "    acc = accuracy_score(y_test, np.where(preds >= 0.5, 1, 0))\n",
        "    auc = roc_auc_score(y_test, preds)\n",
        "\n",
        "    return auc\n",
        "\n",
        "sampler = TPESampler(seed=42)\n",
        "study = optuna.create_study(\n",
        "    study_name=\"lgbm_parameter_opt\",\n",
        "    direction=\"maximize\",\n",
        "    sampler=sampler,\n",
        ")\n",
        "study.optimize(objective, n_trials=10)\n",
        "print(\"Best Score:\", study.best_value)\n",
        "print(\"Best trial:\", study.best_trial.params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.690738Z",
          "start_time": "2021-05-24T09:49:28.989Z"
        },
        "id": "CKRjM0rxhfQ2"
      },
      "outputs": [],
      "source": [
        "# INSTALL MATPLOTLIB IN ADVANCE\n",
        "# _ = lgb.plot_importance(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bsff1ZVhfQ3"
      },
      "source": [
        "## 5. Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.691738Z",
          "start_time": "2021-05-24T09:49:28.992Z"
        },
        "id": "N6YEFm8IhfQ3"
      },
      "outputs": [],
      "source": [
        "# LOAD TESTDATA\n",
        "test_csv_file_path = os.path.join(data_dir, 'test_data.csv')\n",
        "test_df = pd.read_csv(test_csv_file_path)\n",
        "\n",
        "# FEATURE ENGINEERING\n",
        "test_df = feature_engineering(test_df)\n",
        "\n",
        "# LEAVE LAST INTERACTION ONLY\n",
        "test_df = test_df[test_df['userID'] != test_df['userID'].shift(-1)]\n",
        "\n",
        "# DROP ANSWERCODE\n",
        "test_df = test_df.drop(['answerCode'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.692739Z",
          "start_time": "2021-05-24T09:49:28.993Z"
        },
        "id": "XnwXJs_shfQ4"
      },
      "outputs": [],
      "source": [
        "# MAKE PREDICTION\n",
        "total_preds = model.predict(test_df[FEATS])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.694736Z",
          "start_time": "2021-05-24T09:49:28.995Z"
        },
        "id": "f8PvohzwhfQ4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "writing prediction : output/submission.csv\n"
          ]
        }
      ],
      "source": [
        "# SAVE OUTPUT\n",
        "output_dir = 'output/'\n",
        "write_path = os.path.join(output_dir, \"submission.csv\")\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "with open(write_path, 'w', encoding='utf8') as w:\n",
        "    print(\"writing prediction : {}\".format(write_path))\n",
        "    w.write(\"id,prediction\\n\")\n",
        "    for id, p in enumerate(total_preds):\n",
        "        w.write('{},{}\\n'.format(id,p))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
